{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils import model_zoo\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "__print__ = print\n",
    "\n",
    "def print(string):\n",
    "    os.system(f'echo \\\"{string}\\\"')\n",
    "    __print__(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INPUT = '/home/rubn/Desktop/Datasets/Plant'\n",
    "\n",
    "SEED = 42\n",
    "N_FOLDS = 4\n",
    "N_EPOCHS = 12\n",
    "BATCH_SIZE = 10\n",
    "SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = EfficientNet.from_pretrained('efficientnet-b7', num_classes=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, transforms=None):\n",
    "    \n",
    "        self.df = df\n",
    "        self.transforms=transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_src = DIR_INPUT + '/images/' + self.df.loc[idx, 'image_id'] + '.jpg'\n",
    "        # print(image_src)\n",
    "        image = cv2.imread(image_src, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        labels = self.df.loc[idx, ['healthy', 'multiple_diseases', 'rust', 'scab']].values\n",
    "        labels = torch.from_numpy(labels.astype(np.int8))\n",
    "        labels = labels.unsqueeze(-1)\n",
    "        \n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image)\n",
    "            image = transformed['image']\n",
    "\n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass PlantModel(nn.Module):\\n    \\n    def __init__(self, num_classes=4):\\n        super().__init__()\\n        \\n        self.backbone = torchvision.models.resnet18(pretrained=True)\\n        \\n        in_features = self.backbone.fc.in_features\\n\\n        self.logit = nn.Linear(in_features, num_classes)\\n        \\n    def forward(self, x):\\n        batch_size, C, H, W = x.shape\\n        \\n        x = self.backbone.conv1(x)\\n        x = self.backbone.bn1(x)\\n        x = self.backbone.relu(x)\\n        x = self.backbone.maxpool(x)\\n\\n        x = self.backbone.layer1(x)\\n        x = self.backbone.layer2(x)\\n        x = self.backbone.layer3(x)\\n        x = self.backbone.layer4(x)\\n        \\n        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\\n        x = F.dropout(x, 0.25, self.training)\\n\\n        x = self.logit(x)\\n\\n        return x\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class PlantModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        in_features = self.backbone.fc.in_features\n",
    "\n",
    "        self.logit = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, C, H, W = x.shape\n",
    "        \n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        \n",
    "        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n",
    "        x = F.dropout(x, 0.25, self.training)\n",
    "\n",
    "        x = self.logit(x)\n",
    "\n",
    "        return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Residual Bottleneck Block\n",
    "    Args:\n",
    "        block_args (namedtuple): BlockArgs, see above\n",
    "        global_params (namedtuple): GlobalParam, see above\n",
    "    Attributes:\n",
    "        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_args, global_params):\n",
    "        super().__init__()\n",
    "        self._block_args = block_args\n",
    "        self._bn_mom = 1 - global_params.batch_norm_momentum\n",
    "        self._bn_eps = global_params.batch_norm_epsilon\n",
    "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
    "        self.id_skip = block_args.id_skip  # skip connection and drop connect\n",
    "\n",
    "        # Get static or dynamic convolution depending on image size\n",
    "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
    "\n",
    "        # Expansion phase\n",
    "        inp = self._block_args.input_filters  # number of input channels\n",
    "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = self._block_args.kernel_size\n",
    "        s = self._block_args.stride\n",
    "        self._depthwise_conv = Conv2d(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
    "            kernel_size=k, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n",
    "            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Output phase\n",
    "        final_oup = self._block_args.output_filters\n",
    "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "        self._swish = MemoryEfficientSwish()\n",
    "\n",
    "    def forward(self, inputs, drop_connect_rate=None):\n",
    "        \"\"\"\n",
    "        :param inputs: input tensor\n",
    "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
    "        :return: output of block\n",
    "        \"\"\"\n",
    "\n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            x = self._swish(self._bn0(self._expand_conv(inputs)))\n",
    "        x = self._swish(self._bn1(self._depthwise_conv(x)))\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_expand(self._swish(self._se_reduce(x_squeezed)))\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "\n",
    "        x = self._bn2(self._project_conv(x))\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
    "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x\n",
    "\n",
    "    def set_swish(self, memory_efficient=True):\n",
    "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export)\"\"\"\n",
    "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    \"\"\"\n",
    "    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n",
    "    Args:\n",
    "        blocks_args (list): A list of BlockArgs to construct blocks\n",
    "        global_params (namedtuple): A set of GlobalParams shared between blocks\n",
    "    Example:\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, blocks_args=None, global_params=None):\n",
    "        super().__init__()\n",
    "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
    "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
    "        self._global_params = global_params\n",
    "        self._blocks_args = blocks_args\n",
    "\n",
    "        # Get static or dynamic convolution depending on image size\n",
    "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
    "\n",
    "        # Batch norm parameters\n",
    "        bn_mom = 1 - self._global_params.batch_norm_momentum\n",
    "        bn_eps = self._global_params.batch_norm_epsilon\n",
    "\n",
    "        # Stem\n",
    "        in_channels = 3  # rgb\n",
    "        out_channels = round_filters(32, self._global_params)  # number of output channels\n",
    "        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Build blocks\n",
    "        self._blocks = nn.ModuleList([])\n",
    "        for block_args in self._blocks_args:\n",
    "\n",
    "            # Update block input and output filters based on depth multiplier.\n",
    "            block_args = block_args._replace(\n",
    "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
    "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
    "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
    "            )\n",
    "\n",
    "            # The first block needs to take care of stride and filter size increase.\n",
    "            self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
    "            if block_args.num_repeat > 1:\n",
    "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
    "            for _ in range(block_args.num_repeat - 1):\n",
    "                self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
    "\n",
    "        # Head\n",
    "        in_channels = block_args.output_filters  # output of final block\n",
    "        out_channels = round_filters(1280, self._global_params)\n",
    "        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Final linear layer\n",
    "        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self._dropout = nn.Dropout(self._global_params.dropout_rate)\n",
    "        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
    "        self._swish = MemoryEfficientSwish()\n",
    "\n",
    "    def set_swish(self, memory_efficient=True):\n",
    "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export)\"\"\"\n",
    "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n",
    "        for block in self._blocks:\n",
    "            block.set_swish(memory_efficient)\n",
    "\n",
    "\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "\n",
    "        # Stem\n",
    "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):\n",
    "            drop_connect_rate = self._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "\n",
    "        # Head\n",
    "        x = self._swish(self._bn1(self._conv_head(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
    "        bs = inputs.size(0)\n",
    "        # Convolution layers\n",
    "        x = self.extract_features(inputs)\n",
    "\n",
    "        # Pooling and final linear layer\n",
    "        x = self._avg_pooling(x)\n",
    "        x = x.view(bs, -1)\n",
    "        x = self._dropout(x)\n",
    "        x = self._fc(x)\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, model_name, override_params=None):\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        blocks_args, global_params = get_model_params(model_name, override_params)\n",
    "        return cls(blocks_args, global_params)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name, advprop=False, num_classes=4, in_channels=3):\n",
    "        model = cls.from_name(model_name, override_params={'num_classes': num_classes})\n",
    "        load_pretrained_weights(model, model_name, load_fc=(num_classes == 4), advprop=advprop)\n",
    "        if in_channels != 3:\n",
    "            Conv2d = get_same_padding_conv2d(image_size = model._global_params.image_size)\n",
    "            out_channels = round_filters(32, model._global_params)\n",
    "            model._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        return model\n",
    "    \n",
    "    @classmethod\n",
    "    def get_image_size(cls, model_name):\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        _, _, res, _ = efficientnet_params(model_name)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _check_model_name_is_valid(cls, model_name):\n",
    "        \"\"\" Validates model name. \"\"\" \n",
    "        valid_models = ['efficientnet-b'+str(i) for i in range(9)]\n",
    "        if model_name not in valid_models:\n",
    "            raise ValueError('model_name should be one of: ' + ', '.join(valid_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = A.Compose([\n",
    "    A.RandomResizedCrop(height=SIZE, width=SIZE, p=1.0),\n",
    "    A.Flip(),\n",
    "    A.ShiftScaleRotate(rotate_limit=1.0, p=0.8),\n",
    "\n",
    "    # Pixels\n",
    "    A.OneOf([\n",
    "        A.IAAEmboss(p=1.0),\n",
    "        A.IAASharpen(p=1.0),\n",
    "        A.Blur(p=1.0),\n",
    "    ], p=0.5),\n",
    "\n",
    "    # Affine\n",
    "    A.OneOf([\n",
    "        A.ElasticTransform(p=1.0),\n",
    "        A.IAAPiecewiseAffine(p=1.0)\n",
    "    ], p=0.5),\n",
    "\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0),\n",
    "])\n",
    "\n",
    "transforms_valid = A.Compose([\n",
    "    A.Resize(height=SIZE, width=SIZE, p=1.0),\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(p=1.0),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  healthy  multiple_diseases  rust  scab\n",
       "0   Test_0        0                  0     0     0\n",
       "1   Test_1        0                  0     0     0\n",
       "2   Test_2        0                  0     0     0\n",
       "3   Test_3        0                  0     0     0\n",
       "4   Test_4        0                  0     0     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.read_csv(DIR_INPUT + '/sample_submission.csv')\n",
    "submission_df.iloc[:, 1:] = 0\n",
    "\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = PlantDataset(df=submission_df, transforms=transforms_valid)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  healthy  multiple_diseases  rust  scab\n",
       "0  Train_0        0                  0     0     1\n",
       "1  Train_1        0                  1     0     0\n",
       "2  Train_2        1                  0     0     0\n",
       "3  Train_3        0                  0     1     0\n",
       "4  Train_4        1                  0     0     0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(DIR_INPUT + '/train.csv')\n",
    "\n",
    "# For debugging.\n",
    "# train_df = train_df.sample(n=100)\n",
    "# train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_labels = train_df.iloc[:, 1:].values\n",
    "\n",
    "# Need for the StratifiedKFold split\n",
    "train_y = train_labels[:, 2] + train_labels[:, 3] * 2 + train_labels[:, 1] * 3\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "oof_preds = np.zeros((train_df.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained weights.\n",
    "# model = PlantModel(num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseCrossEntropy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DenseCrossEntropy, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        logits = logits.float()\n",
    "        labels = labels.float()\n",
    "        \n",
    "        logprobs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        loss = labels * logprobs\n",
    "        loss = loss.sum(-1)\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(i_fold, model, criterion, optimizer, dataloader_train, dataloader_valid):\n",
    "    \n",
    "    train_fold_results = []\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        # print('  Epoch {}/{}'.format(epoch + 1, N_EPOCHS))\n",
    "        # print('  ' + ('-' * 20))\n",
    "        os.system(f'echo \\\"  Epoch {epoch}\\\"')\n",
    "\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(dataloader_train):\n",
    "\n",
    "            images = batch[0]\n",
    "            labels = batch[1]\n",
    "\n",
    "            images = images.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.squeeze(-1))                \n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = None\n",
    "        val_labels = None\n",
    "\n",
    "        for step, batch in enumerate(dataloader_valid):\n",
    "\n",
    "            images = batch[0]\n",
    "            labels = batch[1]\n",
    "\n",
    "            if val_labels is None:\n",
    "                val_labels = labels.clone().squeeze(-1)\n",
    "            else:\n",
    "                val_labels = torch.cat((val_labels, labels.squeeze(-1)), dim=0)\n",
    "\n",
    "            images = images.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "\n",
    "                loss = criterion(outputs, labels.squeeze(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.softmax(outputs, dim=1).data.cpu()\n",
    "\n",
    "                if val_preds is None:\n",
    "                    val_preds = preds\n",
    "                else:\n",
    "                    val_preds = torch.cat((val_preds, preds), dim=0)\n",
    "\n",
    "\n",
    "        train_fold_results.append({\n",
    "            'fold': i_fold,\n",
    "            'epoch': epoch,\n",
    "            'train_loss': tr_loss / len(dataloader_train),\n",
    "            'valid_loss': val_loss / len(dataloader_valid),\n",
    "            'valid_score': roc_auc_score(val_labels, val_preds, average='macro'),\n",
    "        })\n",
    "\n",
    "    return val_preds, train_fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/4\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "blocks_args should be a list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-635691e5ce20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d7262dc8c0ea>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks_args, global_params)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blocks_args should be a list'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'block args must be greater than 0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: blocks_args should be a list"
     ]
    }
   ],
   "source": [
    "submissions = None\n",
    "train_results = []\n",
    "\n",
    "for i_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, train_y)):\n",
    "    print(\"Fold {}/{}\".format(i_fold + 1, N_FOLDS))\n",
    "\n",
    "    valid = train_df.iloc[valid_idx]\n",
    "    valid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train = train_df.iloc[train_idx]\n",
    "    train.reset_index(drop=True, inplace=True)    \n",
    "\n",
    "    dataset_train = PlantDataset(df=train, transforms=transforms_train)\n",
    "    dataset_valid = PlantDataset(df=valid, transforms=transforms_valid)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
    "    dataloader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE, num_workers=4, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    model = EfficientNet()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = DenseCrossEntropy()\n",
    "    plist = [{'params': model.parameters(), 'lr': 5e-5}]\n",
    "    optimizer = optim.Adam(plist, lr=5e-5)\n",
    "    \n",
    "    val_preds, train_fold_results = train_one_fold(i_fold, model, criterion, optimizer, dataloader_train, dataloader_valid)\n",
    "    oof_preds[valid_idx, :] = val_preds.numpy()\n",
    "    \n",
    "    train_results = train_results + train_fold_results\n",
    "\n",
    "    model.eval()\n",
    "    test_preds = None\n",
    "\n",
    "    for step, batch in enumerate(dataloader_test):\n",
    "\n",
    "        images = batch[0]\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "            if test_preds is None:\n",
    "                test_preds = outputs.data.cpu()\n",
    "            else:\n",
    "                test_preds = torch.cat((test_preds, outputs.data.cpu()), dim=0)\n",
    "    \n",
    "    \n",
    "    # Save predictions per fold\n",
    "    submission_df[['healthy', 'multiple_diseases', 'rust', 'scab']] = torch.softmax(test_preds, dim=1)\n",
    "    submission_df.to_csv('submission_fold_{}.csv'.format(i_fold), index=False)\n",
    "\n",
    "    # logits avg\n",
    "    if submissions is None:\n",
    "        submissions = test_preds / N_FOLDS\n",
    "    else:\n",
    "        submissions += test_preds / N_FOLDS\n",
    "\n",
    "print(\"5-Folds CV score: {:.4f}\".format(roc_auc_score(train_labels, oof_preds, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = pd.DataFrame(train_results)\n",
    "train_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "colors = [\n",
    "    ('#d32f2f', '#ef5350'),\n",
    "    ('#303f9f', '#5c6bc0'),\n",
    "    ('#00796b', '#26a69a'),\n",
    "    ('#fbc02d', '#ffeb3b'),\n",
    "    ('#5d4037', '#8d6e63'),\n",
    "]\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    data = train_results[train_results['fold'] == i]\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=data['epoch'].values,\n",
    "                             y=data['train_loss'].values,\n",
    "                             mode='lines',\n",
    "                             visible='legendonly' if i > 0 else True,\n",
    "                             line=dict(color=colors[i][0], width=2),\n",
    "                             name='Train loss - Fold #{}'.format(i)),\n",
    "                 row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=data['epoch'],\n",
    "                             y=data['valid_loss'].values,\n",
    "                             mode='lines+markers',\n",
    "                             visible='legendonly' if i > 0 else True,\n",
    "                             line=dict(color=colors[i][1], width=2),\n",
    "                             name='Valid loss - Fold #{}'.format(i)),\n",
    "                 row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=data['epoch'].values,\n",
    "                             y=data['valid_score'].values,\n",
    "                             mode='lines+markers',\n",
    "                             line=dict(color=colors[i][0], width=2),\n",
    "                             name='Valid score - Fold #{}'.format(i),\n",
    "                             showlegend=False),\n",
    "                 row=2, col=1)\n",
    "\n",
    "fig.update_layout({\n",
    "  \"annotations\": [\n",
    "    {\n",
    "      \"x\": 0.225, \n",
    "      \"y\": 1.0, \n",
    "      \"font\": {\"size\": 16}, \n",
    "      \"text\": \"Train / valid losses\", \n",
    "      \"xref\": \"paper\", \n",
    "      \"yref\": \"paper\", \n",
    "      \"xanchor\": \"center\", \n",
    "      \"yanchor\": \"bottom\", \n",
    "      \"showarrow\": False\n",
    "    }, \n",
    "    {\n",
    "      \"x\": 0.775, \n",
    "      \"y\": 1.0, \n",
    "      \"font\": {\"size\": 16}, \n",
    "      \"text\": \"Validation scores\", \n",
    "      \"xref\": \"paper\", \n",
    "      \"yref\": \"paper\", \n",
    "      \"xanchor\": \"center\", \n",
    "      \"yanchor\": \"bottom\", \n",
    "      \"showarrow\": False\n",
    "    }, \n",
    "  ]\n",
    "})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[['healthy', 'multiple_diseases', 'rust', 'scab']] = torch.softmax(submissions, dim=1)\n",
    "submission_df.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
